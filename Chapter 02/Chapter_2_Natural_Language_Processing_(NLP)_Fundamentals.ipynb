{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericmedlock/Building-Conversational-Generative-AI-Apps-with-Langchain-and-GPT/blob/main/Chapter%2002/Chapter_2_Natural_Language_Processing_(NLP)_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "86a8cb5b",
      "metadata": {
        "id": "86a8cb5b",
        "outputId": "5fdc0f82-c4fc-4f68-923a-6b3516824cb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk==3.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "sGepDp3xo46K",
      "metadata": {
        "id": "sGepDp3xo46K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4dc32d-46a8-486b-d170-c5c555a34300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf285c5",
      "metadata": {
        "id": "5bf285c5"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a9313ef3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9313ef3",
        "outputId": "824e88f9-d241-49d4-af7d-83e87c565865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'text', 'into', 'smaller', 'units', ',', 'such', 'as', 'words', ',', 'phrases', ',', 'or', 'symbols', ',', 'known', 'as', 'tokens', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "# Sample text for tokenization\n",
        "text = \"Tokenization is the process of breaking text into smaller units, such as words, phrases, or symbols, known as tokens.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Print the tokens\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2086b4",
      "metadata": {
        "id": "da2086b4"
      },
      "source": [
        "### Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5790e4e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5790e4e5",
        "outputId": "068161db-6aa6-4bad-f2c4-4793fa85aeba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "NLTK is a leading platform for building Python programs to work with human language data.\n",
            "\n",
            "Text after stopword removal:\n",
            "NLTK leading platform building Python programs work human language data .\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords dataset\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Join filtered tokens back into a sentence\n",
        "filtered_text = ' '.join(filtered_tokens)\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(text)\n",
        "print(\"\\nText after stopword removal:\")\n",
        "print(filtered_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fa8ab5",
      "metadata": {
        "id": "b2fa8ab5"
      },
      "source": [
        "### NLP Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e9ca7ccf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9ca7ccf",
        "outputId": "02d2f110-7e5f-4a94-d86f-be3835cdc22a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: This is a sample text, with punctuation and  extra  spaces !\n",
            "Normalized text: this is a sample text with punctuation and extra spaces\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "original_text = \"This is a sample text, with punctuation and  extra  spaces !\"\n",
        "normalized_text = normalize_text(original_text)\n",
        "print(\"Original text:\", original_text)\n",
        "print(\"Normalized text:\", normalized_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fb2269",
      "metadata": {
        "id": "b6fb2269"
      },
      "source": [
        "### Rule-based POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "12467422",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12467422",
        "outputId": "a5da4620-0dca-4c85-e8d7-2e7b5eaffbfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DET'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NOUN'), ('over', 'NN'), ('the', 'DET'), ('lazy', 'NN'), ('dog', 'NN'), ('.', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "tagged_words = []\n",
        "for word in tokens:\n",
        "    if word.lower() in ['the', 'a', 'an']:\n",
        "        tagged_words.append((word, 'DET'))\n",
        "    elif word.endswith('s'):\n",
        "        tagged_words.append((word, 'NOUN'))\n",
        "    else:\n",
        "        tagged_words.append((word, 'NN'))\n",
        "\n",
        "print(tagged_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb5799a",
      "metadata": {
        "id": "3cb5799a"
      },
      "source": [
        "### Probabilistic POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ee554b94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee554b94",
        "outputId": "fa9d687a-6887-40bf-f277-e5718baa587a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "tagged_words = nltk.pos_tag(tokens)\n",
        "print(tagged_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e418e8b",
      "metadata": {
        "id": "1e418e8b"
      },
      "source": [
        "### Rule-based NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbdf9d5",
      "metadata": {
        "id": "3fbdf9d5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def rule_based_ner(text):\n",
        "    entities = []\n",
        "    for match in re.finditer(r'\\b[A-Z][a-z]*\\b', text):\n",
        "        entities.append((match.group(0), 'ORGANIZATION'))\n",
        "    return entities\n",
        "\n",
        "text = \"Apple Inc. is headquartered in Cupertino, California.\"\n",
        "entities = rule_based_ner(text)\n",
        "print(entities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d253798f",
      "metadata": {
        "id": "d253798f"
      },
      "source": [
        "### Machine Learning-based NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64b3f58",
      "metadata": {
        "id": "b64b3f58"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def ml_based_ner(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tagged_words = nltk.pos_tag(tokens)\n",
        "    entities = nltk.chunk.ne_chunk(tagged_words)\n",
        "    return entities\n",
        "\n",
        "text = \"Apple Inc. is headquartered in Cupertino, California.\"\n",
        "entities = ml_based_ner(text)\n",
        "print(entities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad981d1",
      "metadata": {
        "id": "0ad981d1"
      },
      "source": [
        "### NER evaluation metrics and challenges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221306ce",
      "metadata": {
        "id": "221306ce"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Example NER system outputs and ground truth labels\n",
        "predicted_labels = ['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC']\n",
        "true_labels = ['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC']\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ebdf53c",
      "metadata": {
        "id": "0ebdf53c"
      },
      "source": [
        "### N-gram Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac5abc9e",
      "metadata": {
        "id": "ac5abc9e"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load and tokenize text data (e.g., from the Reuters corpus)\n",
        "reuters_text = reuters.raw()\n",
        "tokens = word_tokenize(reuters_text.lower())\n",
        "\n",
        "# Define function to generate N-grams of given order\n",
        "def generate_ngrams(token_list, n):\n",
        "    return list(ngrams(token_list, n))\n",
        "\n",
        "# Example usage:\n",
        "# Unigrams (N=1)\n",
        "unigrams = generate_ngrams(tokens, 1)\n",
        "\n",
        "# Bigrams (N=2)\n",
        "bigrams = generate_ngrams(tokens, 2)\n",
        "\n",
        "# Trigrams (N=3)\n",
        "trigrams = generate_ngrams(tokens, 3)\n",
        "\n",
        "# N-grams (N>3)\n",
        "n = 4\n",
        "ngrams = generate_ngrams(tokens, n)\n",
        "\n",
        "# Print example outputs\n",
        "print(\"Example Unigrams (N=1):\", unigrams[:5])\n",
        "print(\"Example Bigrams (N=2):\", bigrams[:5])\n",
        "print(\"Example Trigrams (N=3):\", trigrams[:5])\n",
        "print(f\"Example {n}-grams (N={n}):\", ngrams[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s1yDNGCSkkBP",
      "metadata": {
        "id": "s1yDNGCSkkBP"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21baa7a9",
      "metadata": {
        "id": "21baa7a9"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import reuters\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "# Load and tokenize text data (e.g., from the Reuters corpus)\n",
        "reuters_text = reuters.raw()\n",
        "sentences = [word_tokenize(word.lower()) for word in sent_tokenize(reuters_text)]\n",
        "\n",
        "# Train Word2Vec model using Skip-gram architecture\n",
        "word2vec_model = Word2Vec(sentences, sg=1, vector_size=100, window=5, min_count=5)\n",
        "\n",
        "# Get vector representation of a word\n",
        "vector = word2vec_model.wv['finance']\n",
        "print(\"Vector representation of 'finance':\", vector)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2axc62qllYG0",
      "metadata": {
        "id": "2axc62qllYG0"
      },
      "source": [
        "### GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P3aIbHOJlXYF",
      "metadata": {
        "id": "P3aIbHOJlXYF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the path to your downloaded GloVe file (adjust filename and dimensionality)\n",
        "glove_file = \"/content/glove.twitter.27B.25d.txt\"\n",
        "\n",
        "# Create a dictionary to store word vectors\n",
        "word_vectors = {}\n",
        "\n",
        "# Read the GloVe file and populate the dictionary\n",
        "with open(glove_file, encoding=\"utf8\") as file:\n",
        "  for line in file:\n",
        "    line_split = line.split()\n",
        "    word = line_split[0]\n",
        "    vector = np.array([float(val) for val in line_split[1:]])\n",
        "    word_vectors[word] = vector\n",
        "\n",
        "# Example usage: Find the vector representation of a word\n",
        "word = \"king\"\n",
        "if word in word_vectors:\n",
        "  word_vector = word_vectors[word]\n",
        "  print(f\"Word: {word}, Vector Shape: {word_vector.shape}\")\n",
        "  # Access individual vector components (example: first 10 elements)\n",
        "  print(f\"First 10 elements: {word_vector[:10]}\")\n",
        "else:\n",
        "  print(f\"Word '{word}' not found in vocabulary\")\n",
        "\n",
        "# Example usage: Calculate word similarity using cosine similarity\n",
        "word1 = \"king\"\n",
        "word2 = \"queen\"\n",
        "if word1 in word_vectors and word2 in word_vectors:\n",
        "  vector1 = word_vectors[word1]\n",
        "  vector2 = word_vectors[word2]\n",
        "  similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "  print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n",
        "else:\n",
        "  print(f\"At least one word not found in vocabulary\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HFSz7LRwdtiU",
      "metadata": {
        "id": "HFSz7LRwdtiU"
      },
      "source": [
        " ### TF-IDF NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18ZbRlm1duJR",
      "metadata": {
        "id": "18ZbRlm1duJR"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample corpus of documents\n",
        "corpus = [\n",
        "    'The cat sat on the mat.',\n",
        "    'The dog jumped over the fence.',\n",
        "    'The cat and the dog are best friends.'\n",
        "]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit-transform the corpus to calculate TF-IDF scores\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get feature names (words) from the vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print TF-IDF scores for each word in the corpus\n",
        "for i, doc in enumerate(corpus):\n",
        "    print(f\"Document {i+1}:\")\n",
        "    for j, word in enumerate(feature_names):\n",
        "        tfidf_score = tfidf_matrix[i, j]\n",
        "        if tfidf_score > 0:\n",
        "            print(f\"   {word}: {tfidf_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KqYfszX-_cGS",
      "metadata": {
        "id": "KqYfszX-_cGS"
      },
      "source": [
        "### Syntax and syntactic analysis in NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ix7ydnilXbJ",
      "metadata": {
        "id": "7ix7ydnilXbJ"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample text for syntactic analysis\n",
        "text = \"The cat sat on the mat.\"\n",
        "\n",
        "# Perform syntactic analysis\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print dependency parse tree\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "            [child for child in token.children])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Rc98X8Tg4ae",
      "metadata": {
        "id": "_Rc98X8Tg4ae"
      },
      "source": [
        "### Dependency parsing vs. constituency parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nFP1thlVg4hS",
      "metadata": {
        "id": "nFP1thlVg4hS"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Sample text for parsing\n",
        "text = \"The cat sat on the mat.\"\n",
        "\n",
        "# Perform dependency parsing\n",
        "doc_dep = nlp(text)\n",
        "print(\"Dependency Parsing:\")\n",
        "for token in doc_dep:\n",
        "    print(token.text, token.dep_, token.head.text)\n",
        "\n",
        "# Perform constituency parsing (using spaCy's noun chunks)\n",
        "doc_const = nlp(text)\n",
        "print(\"\\nConstituency Parsing:\")\n",
        "for chunk in doc_const.noun_chunks:\n",
        "    print(chunk.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Osn8x2RGkFTz",
      "metadata": {
        "id": "Osn8x2RGkFTz"
      },
      "source": [
        "### Word embeddings and semantic similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qvcMcuc3kFfb",
      "metadata": {
        "id": "qvcMcuc3kFfb"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained Word2Vec model\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Example usage: Finding similar words\n",
        "word = \"car\"\n",
        "similar_words = model.most_similar(word)\n",
        "print(f\"Words similar to '{word}':\")\n",
        "for similar_word, similarity_score in similar_words:\n",
        "    print(f\"{similar_word}: {similarity_score}\")\n",
        "\n",
        "print(\"=\" * 35)\n",
        "# Example usage: Word vector arithmetic\n",
        "word1 = \"king\"\n",
        "word2 = \"man\"\n",
        "word3 = \"woman\"\n",
        "\n",
        "# Compute semantic similarity\n",
        "similarity1_2 = model.similarity(word1, word2)\n",
        "similarity1_3 = model.similarity(word1, word3)\n",
        "\n",
        "# Print results\n",
        "print(f\"Semantic similarity between '{word1}' and '{word2}': {similarity1_2}\")\n",
        "print(f\"Semantic similarity between '{word1}' and '{word3}': {similarity1_3}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UXaaLbnSuu2l",
      "metadata": {
        "id": "UXaaLbnSuu2l"
      },
      "source": [
        "### Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cdd9c99",
      "metadata": {
        "id": "1cdd9c99"
      },
      "outputs": [],
      "source": [
        "! pip install spacy ==3.7.5\n",
        "! pip install spacytextblob ==5.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ziaAtcnknajD",
      "metadata": {
        "id": "ziaAtcnknajD"
      },
      "outputs": [],
      "source": [
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zliHka4ioJly",
      "metadata": {
        "id": "zliHka4ioJly"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import spacy\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Add the TextBlob extension to the pipeline\n",
        "nlp.add_pipe('spacytextblob')\n",
        "\n",
        "\n",
        "text = \"good product\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the sentiment scores\n",
        "print('text:', text)\n",
        "print('Polarity:', doc._.blob.polarity)\n",
        "print('Subjectivity:', doc._.blob.subjectivity)\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Process some text\n",
        "text = \"bad product\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the sentiment scores\n",
        "print('text:', text)\n",
        "print('Polarity:', doc._.blob.polarity)\n",
        "print('Subjectivity:', doc._.blob.subjectivity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zVcyaJpWogWQ",
      "metadata": {
        "id": "zVcyaJpWogWQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}